---
title: "The Impact of Non-Normality on the T-Test"
format: 
  html:
    embed-resources: true
execute: 
  error: true
  echo: true
editor: visual
---

github link: <https://github.com/juanitaW12/stats506>

## Introduce

T-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, usually under the assumption that the data follow a normal distribution. If the distribution is not normal, it may increase Type I error, lead to inaccurate p-values, and result in biased confidence intervals, especially to small sample sizes. However, according to the Central Limit Theorem, large sample sizes can mitigate the negative impact of non-normality.

Besides, there are three main types of t-tests: **Independent t-test**, **One-sample t-test**, and **Paired t-test**. Since the focus of this study is on how non-normality affects the overall robustness of t-tests, we will concentrate on the first two types.

To investigate this, I will employ **Monte Carlo simulations** to examine the performance of t-tests under different distributions and sample sizes. It can generate datasets under controlled conditions to evaluate t-test performance systematically.

## Research Questions

Based on the introduction, I split this project into three small research questions.

1.  How does non-normality affect Type I error rates, power, confidence interval coverage, and p-value bias in t-tests?
2.  What sample size is necessary for the Central Limit Theorem to reduce the impact of non-normality in these t-tests?
3.  Are there differences in necessary sample size between the Independent and One-sample t-tests under various distributions?

## Simulation Procedure

Firstly, I generate random samples for the following distribution:

1.  Normal Distribution (Baseline) serves as a benchmark
2.  Exponential Distribution (Positive Skew) to assess the impact of asymmetry to t-test performance
3.  Uniform Distribution (Flat) to assess the performance if without clear central tendencies
4.  Bimodal Distribution to assess the impact of complex structures
5.  Heavy-Tailed Distribution (t-distribution with df = 3) to assess the impact of extreme values

Next, to evaluate the role of the (CLT), simulations will use a range of sample sizes: Small(10,20), Medium(50, 100), Large(500,1000, 2000, 3000).

Final, I will conduct One-sample and Independent t-test with a significance level (α=0.05), repeating 10,000 simulations for each distribution, sample size, and t-test type.

## Code

```{r}
library(MASS)
library(knitr)
```

```{r}
# Set parameters
sample_sizes <- c(10, 20, 50, 100, 500, 1000, 2000)
mu_normal <- 1
```

For reproducible simulations, I will create functions for one-sample T-test simulations and independent T-test simulations separately.

```{r}
# 
#' Title Function for One-Sample T-Test Simulation
#'
#' @param dist_func,params,n,mu,reps,alpha,seed
#' 
#' @return metrics of t-test of 10000
#' @export 
#'
#' @examples
one_sample_ttest_sim <- function(dist_func, params, # distribution params
                                 n, mu = mu_normal, reps = 10000, alpha = 0.05, seed = 123) {
  
  # set seed for productive uotputs
  if (!is.null(seed)) {
    set.seed(seed)
    }
  
  type1_error <- 0 # the original values and then calculate the sum
  power <- 0
  ci_coverage <- 0
  p_values <- numeric(reps) # a vector with a length of reps(10000)
  
  for (i in 1:reps) {
    # generate random samples with distribution function of dist_func and input params 
    sample <- do.call(dist_func, c(list(n = n), params))
    t_result <- t.test(sample, mu = mu)  # one-sample t-test
    
    ci <- t_result$conf.int # get the coverage interval 
    p_values[i] <- t_result$p.value # store all the p values
    
    if (t_result$p.value < alpha) {
      if (mean(sample) == mu) {
        type1_error <- type1_error + 1
      } else {
        power <- power + 1 
      }
    }
        
    if (mu >= ci[1] && mu <= ci[2]) { # check whether ci contains true mean
      ci_coverage <- ci_coverage + 1
    }
  }
  
  p_value_bias <- mean(p_values) - alpha # deviation from alpha(0.05)
  
  # the final result
  final_export <- list(
    Type1_Error = round(type1_error / reps,4),
    Power = round(power / reps, 4),
    CI_Coverage = round(ci_coverage / reps,4),
    P_Value_Bias = round(p_value_bias,4)
    )
  
  return(final_export)
}

```

The function of Independent T-Test Simulation is similar to the function above.

```{r}
# 
#' Title Function for Independent T-Test Simulation
#'
#' @param dist_func,params,n,reps,alpha,seed
#' 
#' @return metrics of t-test of 10000
#' @export 
#'
#' @examples
independent_ttest_sim <- function(dist_func1, params1, dist_func2, params2, # distribution params
                                  n1, n2, reps = 10000, alpha = 0.05, seed = 123) {
  # set seed for productive uotputs
  if (!is.null(seed)) {
    set.seed(seed)
  }
  
  type1_error <- 0 # the original values and then calculate the sum
  power <- 0
  ci_coverage <- 0
  p_values <- numeric(reps) # a vector with a length of reps(10000)
  
  
  for (i in 1:reps) {
    # generate two random samples with specific distribution
    sample1 <- do.call(dist_func1, c(list(n = n1), params1))
    sample2 <- do.call(dist_func2, c(list(n = n2), params2))
    
    t_result <- t.test(sample1, sample2) 
    
    ci <- t_result$conf.int # get the coverage interval 
    p_values[i] <- t_result$p.value # store all the p values
    
    if (t_result$p.value < alpha) {
      if (mean(sample1) == mean(sample2)) {   # Null is true
        type1_error <- type1_error + 1
      } else {  # Null is false
        power <- power + 1
      }
    }
    
    # check whether ci contains true mean
    if ((mean(sample1) - mean(sample2)) >= ci[1] && (mean(sample1) - mean(sample2)) <= ci[2]) { 
      # Confidence interval contains true values
      ci_coverage <- ci_coverage + 1
    }
  }
  
  p_value_bias <- mean(p_values) - alpha 
  
  # the final result
  final_export <- list(
    Type1_Error = round(type1_error / reps,4),
    Power = round(power / reps, 4),
    CI_Coverage = round(ci_coverage / reps,4),
    P_Value_Bias = round(p_value_bias,4)
    )
  
  return(final_export)
}
```

Next, we define the parameters of the distributions.

```{r}
# Distributions to Simulate
distributions <- list(
  Normal = list(func = "rnorm", params = list(mean = 1, sd = 1)),
  Exponential = list(func = "rexp", params = list(rate = 1)),
  Uniform = list(func = "runif", params = list(min = 0, max = 2)),
  Bimodal = list(func = function(n) c(rnorm(n / 2, 0, 1), rnorm(n / 2, 2, 1)), params = list()),
  HeavyTail = list(func = "rt", params = list(df = 3))
)
```

Then, run the functions and get outputs.

```{r}
# Perform Simulations
results_one_sample <- list()
results_independent <- list()

for (dist_name in names(distributions)) {
  dist <- distributions[[dist_name]]
  
  # One-Sample T-Test Simulations
  results_one_sample[[dist_name]] <- lapply(sample_sizes, function(n) {
    one_sample_ttest_sim(dist$func, dist$params, n = n)
  })
  names(results_one_sample[[dist_name]]) <- paste0("n=", sample_sizes)
  
  # Independent T-Test Simulations
  results_independent[[dist_name]] <- lapply(sample_sizes, function(n) {
    independent_ttest_sim(
      dist$func, dist$params, dist$func, dist$params,
      n1 = n, n2 = n, 
    )
  })
  names(results_independent[[dist_name]]) <- paste0("n=", sample_sizes)
}
  
```

```{r}
# Convert results to a data frame for formatting
format_results <- function(results) {
  data <- do.call(rbind, lapply(names(results), function(n) {
    cbind(Sample_Size = n, as.data.frame(results[[n]]))
  }))
  df <- as.data.frame(data)
  return(t(df))
  
}

# Format tables
one_sample_df <- format_results(results_one_sample)
independent_df <- format_results(results_independent)
```

```{r}
# Display 
cat("\n### One-Sample T-Test Results\n")
suppressWarnings(kable(one_sample_df, caption = "One-Sample T-Test Results",
                       digits = 3))

cat("\n### Independent T-Test Results\n")
suppressWarnings(kable(independent_df, caption = "Independent T-Test Results",
                       digits = 3))
```

## Performance

### Metrics Introduction

1.  Type I Error Rate = Number of incorrect rejections / Total number of experiments
2.  Power, Power= Number of correct rejections / Total number of experiments
3.  Confidence Interval Coverage, = Number of confidence intervals covering the true value / Total number of experiments
4.  P-Value Bias = mean(p-value) - α (0.05)

## Reference

1.  The t-test and robustness to non-normality,

    <https://thestatsgeek.com/2013/09/28/the-t-test-and-robustness-to-non-normality/>

2.  ChatGPT to modify codes and polishing my English manuscript.
